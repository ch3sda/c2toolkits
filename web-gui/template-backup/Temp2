        // qwen2.5:7b is a good balance between performance and capability
        //     pro: better understanding of technical content
        //     con: may require more resources, 
        // if you have low software specs, use smaller models like "llama3.2:3b"
        //     pro: lower resource usage
        //     con: may miss nuanced technical details
        // you can change the model based on your Ollama setup
        // Adjust model choice based on your Ollama installation and resource availability